{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import datetime\n",
    "\n",
    "# custom imports\n",
    "sys.path.append('../src')\n",
    "from utils.dataset import load_datasets\n",
    "from utils.foldergen import generate_folder\n",
    "from classes.gan.gan import Generator, Discriminator\n",
    "from classes.gan.ganCNN import GeneratorCNN, DiscriminatorCNN\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasettype = \"MNIST\"\n",
    "\n",
    "# Train a new network or continue training a previously trained network:\n",
    "continueTraining = False;\n",
    "\n",
    "# learning rate\n",
    "lr = 0.0002\n",
    "\n",
    "# number of epochs\n",
    "num_epochs = 100\n",
    "batch_size = 25\n",
    "\n",
    "hidden_dim = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if datasettype == \"MNIST\":\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),  # create PyTorch Tensor | shape: (channels, height, width)\n",
    "            transforms.Normalize(\n",
    "                [0.5], [0.5]\n",
    "            ),  # convert values to [-1, 1]\n",
    "        ]\n",
    "    )\n",
    "elif datasettype == \"CIFAR10\":\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# create folder structure\n",
    "data_folder = \"data\"\n",
    "load_folder = \"load\"\n",
    "output_folder = \"output\"\n",
    "folders = [data_folder, load_folder, output_folder]\n",
    "generate_folder(folders)\n",
    "\n",
    "dataset_train, loader_train = load_datasets(datasettype, transform, batch_size)\n",
    "dataset_test, loader_test = load_datasets(datasettype, transform, batch_size, train=False, download=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 18\u001b[0m\n\u001b[0;32m     13\u001b[0m     data_dim \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m \u001b[39m*\u001b[39m \u001b[39m32\u001b[39m \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[39m# Gen = Generator(g_input_dim= z_dim, g_hidden_dim= hidden_dim, g_output_dim= data_dim,).to(device)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m# Dis = Discriminator(d_input_dim= data_dim,  d_hidden_dim= hidden_dim).to(device)\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m Gen \u001b[39m=\u001b[39m GeneratorCNN(latent_dim \u001b[39m=\u001b[39;49m z_dim, img_channels \u001b[39m=\u001b[39;49m image_channels, img_size \u001b[39m=\u001b[39;49m data_dim)\n\u001b[0;32m     19\u001b[0m Dis \u001b[39m=\u001b[39m DiscriminatorCNN(img_channels \u001b[39m=\u001b[39m image_channels, img_size \u001b[39m=\u001b[39m data_dim)\n\u001b[0;32m     21\u001b[0m lossFunction \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCELoss()\n",
      "File \u001b[1;32mc:\\Users\\leonk\\Documents\\GitHub\\PyTorch-Project\\GAN\\../src\\classes\\gan\\ganCNN.py:13\u001b[0m, in \u001b[0;36mGeneratorCNN.__init__\u001b[1;34m(self, latent_dim, img_channels, img_size)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_size \u001b[39m=\u001b[39m img_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m4\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_dim \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_size\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmain \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m---> 13\u001b[0m     nn\u001b[39m.\u001b[39;49mLinear(latent_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear_dim),\n\u001b[0;32m     14\u001b[0m     nn\u001b[39m.\u001b[39mReLU(\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m     15\u001b[0m     nn\u001b[39m.\u001b[39mBatchNorm1d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_dim),\n\u001b[0;32m     16\u001b[0m     nn\u001b[39m.\u001b[39mUnflatten(\u001b[39m1\u001b[39m, (\u001b[39m256\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_size)),\n\u001b[0;32m     17\u001b[0m     nn\u001b[39m.\u001b[39mConvTranspose2d(\u001b[39m256\u001b[39m, \u001b[39m128\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m     18\u001b[0m     nn\u001b[39m.\u001b[39mBatchNorm2d(\u001b[39m128\u001b[39m),\n\u001b[0;32m     19\u001b[0m     nn\u001b[39m.\u001b[39mReLU(\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m     20\u001b[0m     nn\u001b[39m.\u001b[39mConvTranspose2d(\u001b[39m128\u001b[39m, img_channels, kernel_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m     21\u001b[0m     nn\u001b[39m.\u001b[39mTanh(),\n\u001b[0;32m     22\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\leonk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n",
      "File \u001b[1;32mc:\\Users\\leonk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[39m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[39m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     \u001b[39m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[0;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
      "File \u001b[1;32mc:\\Users\\leonk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[1;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[0;32m    410\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "z_dim = 100\n",
    "# automatically calculate the dimension -> does not work with CIFAR10\n",
    "# data_dim = 1\n",
    "# for dimension in range(1, dataset_train.data.ndim):\n",
    "#     data_dim *= dataset_train.data.size(dimension)\n",
    "\n",
    "# set image_channels depending on datasettype\n",
    "if datasettype == \"MNIST\":\n",
    "    image_channels = 0\n",
    "    data_dim = 28 * 28\n",
    "elif datasettype == \"CIFAR10\":\n",
    "    image_channels = 3\n",
    "    data_dim = 32 * 32 * 3\n",
    "\n",
    "Gen = Generator(g_input_dim= z_dim, g_hidden_dim= hidden_dim, g_output_dim= data_dim,).to(device)\n",
    "Dis = Discriminator(d_input_dim= data_dim,  d_hidden_dim= hidden_dim, data_dim=data_dim).to(device)\n",
    "\n",
    "# Gen = GeneratorCNN(latent_dim = z_dim, img_channels = image_channels, img_size = data_dim)\n",
    "# Dis = DiscriminatorCNN(img_channels = image_channels, img_size = data_dim)\n",
    "\n",
    "lossFunction = nn.BCELoss()\n",
    "\n",
    "# Optimizers\n",
    "Gen_optimizer = optim.Adam(Gen.parameters(), lr=lr)\n",
    "Dis_optimizer = optim.Adam(Dis.parameters(), lr=lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dis_train(x):\n",
    "    Dis.zero_grad()\n",
    "\n",
    "    # real data\n",
    "    y_real = torch.ones(batch_size, 1)\n",
    "    x_real, y_real = x.to(device), y_real.to(device)\n",
    "\n",
    "    D_output = Dis(x_real)\n",
    "    D_real_loss = lossFunction(D_output, y_real)\n",
    "\n",
    "    # fake data\n",
    "    z =  torch.randn(batch_size, z_dim).to(device)\n",
    "    x_fake, y_fake = Gen(z), torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "    D_output = Dis(x_fake)\n",
    "    D_fake_loss = lossFunction(D_output, y_fake)\n",
    "\n",
    "    # loss\n",
    "    D_loss = D_real_loss + D_fake_loss\n",
    "    D_loss.backward()\n",
    "    Dis_optimizer.step()\n",
    "\n",
    "    return D_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gen_train(x):\n",
    "    Gen.zero_grad()\n",
    "\n",
    "    z = torch.randn(batch_size, z_dim).to(device)\n",
    "    y = torch.ones(batch_size, 1).to(device)\n",
    "\n",
    "    G_output = Gen(z)\n",
    "    D_output = Dis(G_output)\n",
    "    G_loss = lossFunction(D_output, y)\n",
    "\n",
    "    # loss\n",
    "    G_loss.backward()\n",
    "    Gen_optimizer.step()\n",
    "\n",
    "    return G_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for further training\n",
    "\n",
    "discriminator_file = \"discriminator.pth\"\n",
    "generator_file = \"generator.pth\"\n",
    "\n",
    "# Check if directory load and files for discriminator and generator exist\n",
    "if os.path.exists(load_folder) and os.path.isfile(os.path.join(load_folder, discriminator_file)) and os.path.isfile(os.path.join(load_folder, generator_file)) and continueTraining:\n",
    "    Gen = Generator(g_input_dim = z_dim, g_hidden_dim=hidden_dim, g_output_dim = data_dim).to(device)\n",
    "    Gen.load_state_dict(torch.load(os.path.join(load_folder, generator_file), map_location=device))\n",
    "\n",
    "    Dis = Discriminator(d_input_dim= data_dim, d_hidden_dim= hidden_dim).to(device)\n",
    "    Dis.load_state_dict(torch.load(os.path.join(load_folder, discriminator_file), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [25, 784]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m D_losses, G_losses \u001b[39m=\u001b[39m [], []\n\u001b[0;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (x, _) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(loader_train):\n\u001b[1;32m---> 19\u001b[0m     D_losses\u001b[39m.\u001b[39mappend(Dis_train(x))\n\u001b[0;32m     20\u001b[0m     G_losses\u001b[39m.\u001b[39mappend(Gen_train(x))\n\u001b[0;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m[\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m]: loss_d: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m, loss_g: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\n\u001b[0;32m     23\u001b[0m         (epoch), num_epochs, torch\u001b[39m.\u001b[39mmean(torch\u001b[39m.\u001b[39mFloatTensor(D_losses)), torch\u001b[39m.\u001b[39mmean(torch\u001b[39m.\u001b[39mFloatTensor(G_losses))))\n",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m, in \u001b[0;36mDis_train\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      5\u001b[0m x_real, y_real \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, data_dim), torch\u001b[39m.\u001b[39mones(batch_size, \u001b[39m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m x_real, y_real \u001b[39m=\u001b[39m x_real\u001b[39m.\u001b[39mto(device), y_real\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 8\u001b[0m D_output \u001b[39m=\u001b[39m Dis(x_real)\n\u001b[0;32m      9\u001b[0m D_real_loss \u001b[39m=\u001b[39m lossFunction(D_output, y_real)\n\u001b[0;32m     11\u001b[0m \u001b[39m# fake data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\leonk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\leonk\\Documents\\GitHub\\PyTorch-Project\\GAN\\../src\\classes\\gan\\ganCNN.py:52\u001b[0m, in \u001b[0;36mDiscriminatorCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 52\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmain(x)\n\u001b[0;32m     53\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     54\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(x)\n",
      "File \u001b[1;32mc:\\Users\\leonk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\leonk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\leonk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\leonk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\leonk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [25, 784]"
     ]
    }
   ],
   "source": [
    "# create folder structure if it does not exist\n",
    "current_time = datetime.datetime.now()\n",
    "formatted_time = current_time.strftime(\"%d%m-%H%M\")\n",
    "\n",
    "# Create subfolders if they don't exist\n",
    "pictures_folder = os.path.join(output_folder, formatted_time, \"pic\")\n",
    "model_folder = os.path.join(output_folder, formatted_time, \"model\")\n",
    "\n",
    "if not os.path.exists(pictures_folder):\n",
    "    os.makedirs(pictures_folder)\n",
    "\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    D_losses, G_losses = [], []\n",
    "    for batch_idx, (x, _) in enumerate(loader_train):\n",
    "        D_losses.append(Dis_train(x))\n",
    "        G_losses.append(Gen_train(x))\n",
    "\n",
    "    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n",
    "            (epoch), num_epochs, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses))))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if epoch % 10 == 0:\n",
    "            test_z = Variable(torch.randn(batch_size, z_dim).to(device))\n",
    "            generated = Gen(test_z)\n",
    "\n",
    "            # format output string\n",
    "            \n",
    "            formatted_number = \"{:0{}}\".format(epoch, len(str(num_epochs)))\n",
    "\n",
    "            save_image(generated.view(generated.size(0), 1, 28, 28), pictures_folder + '/' + formatted_number + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_z = torch.randn(batch_size, z_dim).to(device)\n",
    "    generated = Gen(test_z)\n",
    "\n",
    "    save_image(generated.view(generated.size(0), 1, 28, 28), pictures_folder + '/' + 'final.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Gen.state_dict(), model_folder + '/' + 'generator.pth')\n",
    "torch.save(Dis.state_dict(), model_folder + '/' + 'discriminator.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

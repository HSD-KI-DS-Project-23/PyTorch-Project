{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Von Maximilian Horster und Yanik Plutte"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Generall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available: False \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Cuda is available:',torch.cuda.is_available(),'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size = (8*4*4)\n",
    "hidden_dim = 128\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "num_epochs = 8\n",
    "learn_rate = 0.0003\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder structure if it does not exist\n",
    "folder_path = \"../../data/\"\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "\n",
    "#MNIST Dataset\n",
    "    #Create Folder, transorm to Tensor\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=folder_path, train=True, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.01307),(0.3081))]), download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=folder_path, train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.01307),(0.3081))])\n",
    ")\n",
    "\n",
    "#Data Loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True #use shuffle to not let the network train on the order of data\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "#show MNIST Dataset\n",
    "\n",
    "# figure= plt.figure(figsize=(10,8))\n",
    "# spalten, zeilen = 5,5\n",
    "# for i in range(1,spalten*zeilen+1):\n",
    "#     sample_idx = torch.randint(len(train_dataset),size=(1,)).item()\n",
    "#     image , label = train_dataset[sample_idx]\n",
    "#     figure.add_subplot(spalten,zeilen,i)\n",
    "#     plt.title(label, weight='bold')\n",
    "#     plt.imshow(image.squeeze(), cmap='gray')\n",
    "# plt.subplots_adjust(wspace=0.3,hspace=0.6)\n",
    "# plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary models for classification\n",
    "    # reduce complexitiy and mass of code\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from classes.autoencoder.autoencoderCNN import AutoEncoder\n",
    "from classes.imageclassification.classification import MNIST_Classification_Class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Load AE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define AutoEncoder Model and use encoder\n",
    "\n",
    "ModelAE = AutoEncoder().to(device)\n",
    "\n",
    "ModelAE.eval()\n",
    "\n",
    "trained_encoder = ModelAE.encode\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Load Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define Image classification model \n",
    "\n",
    "Model_Classifier = MNIST_Classification_Class(input_size, hidden_dim).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  6.0 Function: View Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_classifications(img,pred):\n",
    "    pred = pred.cpu().data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(figsize=(6,9), ncols=2) \n",
    "    ax1.imshow(img.resize_(1, 28, 28).cpu().numpy().squeeze())\n",
    "    ax1.set_title('Original MNIST Image')\n",
    "    \n",
    "    bars = ax2.barh(np.arange(10), pred)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))               #Create bar graph to display prediction probability\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    for bar, val in zip(bars, pred):\n",
    "        if not (val < 0.3):\n",
    "            ax2.text(val, bar.get_y() + bar.get_height() / 2, round(val, 2), va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 1 of 7500.0 in Epoche: 1 // Loss: 2.29299 \n",
      "Image: 513 of 7500.0 in Epoche: 1 // Loss: 2.12798 \n",
      "Image: 1025 of 7500.0 in Epoche: 1 // Loss: 2.14402 \n",
      "Image: 1537 of 7500.0 in Epoche: 1 // Loss: 1.86320 \n",
      "Image: 2049 of 7500.0 in Epoche: 1 // Loss: 1.93002 \n",
      "Image: 2561 of 7500.0 in Epoche: 1 // Loss: 1.88088 \n",
      "Image: 3073 of 7500.0 in Epoche: 1 // Loss: 1.70420 \n",
      "Image: 3585 of 7500.0 in Epoche: 1 // Loss: 1.67653 \n",
      "Image: 4097 of 7500.0 in Epoche: 1 // Loss: 1.74796 \n",
      "Image: 4609 of 7500.0 in Epoche: 1 // Loss: 1.72289 \n",
      "Image: 5121 of 7500.0 in Epoche: 1 // Loss: 1.71482 \n",
      "Image: 5633 of 7500.0 in Epoche: 1 // Loss: 1.82349 \n",
      "Image: 6145 of 7500.0 in Epoche: 1 // Loss: 1.93902 \n",
      "Image: 6657 of 7500.0 in Epoche: 1 // Loss: 1.96168 \n",
      "Image: 7169 of 7500.0 in Epoche: 1 // Loss: 1.67108 \n",
      "Epoche: 1 Loss: 1.77742 \n",
      "Image: 1 of 7500.0 in Epoche: 2 // Loss: 1.78798 \n",
      "Image: 513 of 7500.0 in Epoche: 2 // Loss: 1.66863 \n",
      "Image: 1025 of 7500.0 in Epoche: 2 // Loss: 1.64904 \n",
      "Image: 1537 of 7500.0 in Epoche: 2 // Loss: 1.80739 \n",
      "Image: 2049 of 7500.0 in Epoche: 2 // Loss: 1.74743 \n",
      "Image: 2561 of 7500.0 in Epoche: 2 // Loss: 1.95502 \n",
      "Image: 3073 of 7500.0 in Epoche: 2 // Loss: 1.73113 \n",
      "Image: 3585 of 7500.0 in Epoche: 2 // Loss: 1.78488 \n",
      "Image: 4097 of 7500.0 in Epoche: 2 // Loss: 1.67333 \n",
      "Image: 4609 of 7500.0 in Epoche: 2 // Loss: 1.74942 \n",
      "Image: 5121 of 7500.0 in Epoche: 2 // Loss: 1.81369 \n",
      "Image: 5633 of 7500.0 in Epoche: 2 // Loss: 1.68328 \n",
      "Image: 6145 of 7500.0 in Epoche: 2 // Loss: 1.60589 \n",
      "Image: 6657 of 7500.0 in Epoche: 2 // Loss: 1.66252 \n",
      "Image: 7169 of 7500.0 in Epoche: 2 // Loss: 1.62309 \n",
      "Epoche: 2 Loss: 1.73140 \n",
      "Image: 1 of 7500.0 in Epoche: 3 // Loss: 1.65575 \n",
      "Image: 513 of 7500.0 in Epoche: 3 // Loss: 1.75209 \n",
      "Image: 1025 of 7500.0 in Epoche: 3 // Loss: 1.83425 \n",
      "Image: 1537 of 7500.0 in Epoche: 3 // Loss: 1.77545 \n",
      "Image: 2049 of 7500.0 in Epoche: 3 // Loss: 1.76116 \n",
      "Image: 2561 of 7500.0 in Epoche: 3 // Loss: 1.62211 \n",
      "Image: 3073 of 7500.0 in Epoche: 3 // Loss: 1.71599 \n",
      "Image: 3585 of 7500.0 in Epoche: 3 // Loss: 1.76013 \n",
      "Image: 4097 of 7500.0 in Epoche: 3 // Loss: 1.76918 \n",
      "Image: 4609 of 7500.0 in Epoche: 3 // Loss: 1.79155 \n",
      "Image: 5121 of 7500.0 in Epoche: 3 // Loss: 1.82005 \n",
      "Image: 5633 of 7500.0 in Epoche: 3 // Loss: 1.60134 \n",
      "Image: 6145 of 7500.0 in Epoche: 3 // Loss: 1.76638 \n",
      "Image: 6657 of 7500.0 in Epoche: 3 // Loss: 1.87237 \n",
      "Image: 7169 of 7500.0 in Epoche: 3 // Loss: 1.84893 \n",
      "Epoche: 3 Loss: 1.77776 \n",
      "Image: 1 of 7500.0 in Epoche: 4 // Loss: 1.78454 \n",
      "Image: 513 of 7500.0 in Epoche: 4 // Loss: 1.77793 \n",
      "Image: 1025 of 7500.0 in Epoche: 4 // Loss: 1.69656 \n",
      "Image: 1537 of 7500.0 in Epoche: 4 // Loss: 1.83638 \n",
      "Image: 2049 of 7500.0 in Epoche: 4 // Loss: 1.77129 \n",
      "Image: 2561 of 7500.0 in Epoche: 4 // Loss: 1.80539 \n",
      "Image: 3073 of 7500.0 in Epoche: 4 // Loss: 1.65019 \n",
      "Image: 3585 of 7500.0 in Epoche: 4 // Loss: 1.96680 \n",
      "Image: 4097 of 7500.0 in Epoche: 4 // Loss: 1.98646 \n",
      "Image: 4609 of 7500.0 in Epoche: 4 // Loss: 1.63604 \n",
      "Image: 5121 of 7500.0 in Epoche: 4 // Loss: 1.97234 \n",
      "Image: 5633 of 7500.0 in Epoche: 4 // Loss: 1.72606 \n",
      "Image: 6145 of 7500.0 in Epoche: 4 // Loss: 1.91050 \n",
      "Image: 6657 of 7500.0 in Epoche: 4 // Loss: 1.51941 \n",
      "Image: 7169 of 7500.0 in Epoche: 4 // Loss: 1.92637 \n",
      "Epoche: 4 Loss: 1.73857 \n",
      "Image: 1 of 7500.0 in Epoche: 5 // Loss: 1.77301 \n",
      "Image: 513 of 7500.0 in Epoche: 5 // Loss: 1.60836 \n",
      "Image: 1025 of 7500.0 in Epoche: 5 // Loss: 1.80050 \n",
      "Image: 1537 of 7500.0 in Epoche: 5 // Loss: 1.54914 \n",
      "Image: 2049 of 7500.0 in Epoche: 5 // Loss: 1.68218 \n",
      "Image: 2561 of 7500.0 in Epoche: 5 // Loss: 1.77626 \n",
      "Image: 3073 of 7500.0 in Epoche: 5 // Loss: 1.70428 \n",
      "Image: 3585 of 7500.0 in Epoche: 5 // Loss: 1.79572 \n",
      "Image: 4097 of 7500.0 in Epoche: 5 // Loss: 1.56740 \n",
      "Image: 4609 of 7500.0 in Epoche: 5 // Loss: 1.58890 \n",
      "Image: 5121 of 7500.0 in Epoche: 5 // Loss: 1.75737 \n",
      "Image: 5633 of 7500.0 in Epoche: 5 // Loss: 1.72345 \n",
      "Image: 6145 of 7500.0 in Epoche: 5 // Loss: 1.54147 \n",
      "Image: 6657 of 7500.0 in Epoche: 5 // Loss: 1.82104 \n",
      "Image: 7169 of 7500.0 in Epoche: 5 // Loss: 1.58788 \n",
      "Epoche: 5 Loss: 1.54964 \n",
      "Image: 1 of 7500.0 in Epoche: 6 // Loss: 1.70762 \n",
      "Image: 513 of 7500.0 in Epoche: 6 // Loss: 1.53356 \n",
      "Image: 1025 of 7500.0 in Epoche: 6 // Loss: 1.74058 \n",
      "Image: 1537 of 7500.0 in Epoche: 6 // Loss: 1.75323 \n",
      "Image: 2049 of 7500.0 in Epoche: 6 // Loss: 1.58865 \n",
      "Image: 2561 of 7500.0 in Epoche: 6 // Loss: 1.69071 \n",
      "Image: 3073 of 7500.0 in Epoche: 6 // Loss: 1.60632 \n",
      "Image: 3585 of 7500.0 in Epoche: 6 // Loss: 1.85041 \n",
      "Image: 4097 of 7500.0 in Epoche: 6 // Loss: 1.53823 \n",
      "Image: 4609 of 7500.0 in Epoche: 6 // Loss: 1.66005 \n",
      "Image: 5121 of 7500.0 in Epoche: 6 // Loss: 1.85750 \n",
      "Image: 5633 of 7500.0 in Epoche: 6 // Loss: 1.52689 \n",
      "Image: 6145 of 7500.0 in Epoche: 6 // Loss: 1.56082 \n",
      "Image: 6657 of 7500.0 in Epoche: 6 // Loss: 1.78762 \n",
      "Image: 7169 of 7500.0 in Epoche: 6 // Loss: 1.63495 \n",
      "Epoche: 6 Loss: 1.50635 \n",
      "Image: 1 of 7500.0 in Epoche: 7 // Loss: 1.68881 \n",
      "Image: 513 of 7500.0 in Epoche: 7 // Loss: 1.52447 \n",
      "Image: 1025 of 7500.0 in Epoche: 7 // Loss: 1.65074 \n",
      "Image: 1537 of 7500.0 in Epoche: 7 // Loss: 1.63841 \n",
      "Image: 2049 of 7500.0 in Epoche: 7 // Loss: 1.79833 \n",
      "Image: 2561 of 7500.0 in Epoche: 7 // Loss: 1.79914 \n",
      "Image: 3073 of 7500.0 in Epoche: 7 // Loss: 1.50706 \n",
      "Image: 3585 of 7500.0 in Epoche: 7 // Loss: 1.54966 \n",
      "Image: 4097 of 7500.0 in Epoche: 7 // Loss: 1.65152 \n",
      "Image: 4609 of 7500.0 in Epoche: 7 // Loss: 1.51892 \n",
      "Image: 5121 of 7500.0 in Epoche: 7 // Loss: 1.67058 \n",
      "Image: 5633 of 7500.0 in Epoche: 7 // Loss: 1.56109 \n",
      "Image: 6145 of 7500.0 in Epoche: 7 // Loss: 1.60297 \n",
      "Image: 6657 of 7500.0 in Epoche: 7 // Loss: 1.64093 \n",
      "Image: 7169 of 7500.0 in Epoche: 7 // Loss: 1.53630 \n",
      "Epoche: 7 Loss: 1.57234 \n",
      "Image: 1 of 7500.0 in Epoche: 8 // Loss: 1.78087 \n",
      "Image: 513 of 7500.0 in Epoche: 8 // Loss: 1.75435 \n",
      "Image: 1025 of 7500.0 in Epoche: 8 // Loss: 1.79807 \n",
      "Image: 1537 of 7500.0 in Epoche: 8 // Loss: 1.78564 \n",
      "Image: 2049 of 7500.0 in Epoche: 8 // Loss: 1.69814 \n",
      "Image: 2561 of 7500.0 in Epoche: 8 // Loss: 1.50205 \n",
      "Image: 3073 of 7500.0 in Epoche: 8 // Loss: 1.49078 \n",
      "Image: 3585 of 7500.0 in Epoche: 8 // Loss: 1.55476 \n",
      "Image: 4097 of 7500.0 in Epoche: 8 // Loss: 1.49356 \n",
      "Image: 4609 of 7500.0 in Epoche: 8 // Loss: 1.71708 \n",
      "Image: 5121 of 7500.0 in Epoche: 8 // Loss: 1.69345 \n",
      "Image: 5633 of 7500.0 in Epoche: 8 // Loss: 1.61434 \n",
      "Image: 6145 of 7500.0 in Epoche: 8 // Loss: 1.61112 \n",
      "Image: 6657 of 7500.0 in Epoche: 8 // Loss: 1.57607 \n",
      "Image: 7169 of 7500.0 in Epoche: 8 // Loss: 1.50814 \n",
      "Epoche: 8 Loss: 1.61063 \n"
     ]
    }
   ],
   "source": [
    "#Load Model / Continue Training\n",
    "continue_training = True\n",
    "\n",
    "PATH = \"classification_model.pth\"\n",
    "\n",
    "# if continue_training:\n",
    "#     ModelClassification = torch.load(PATH).to(device)\n",
    "\n",
    "#criterion / lf\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = optim.Adam(params=Model_Classifier.parameters(),lr=learn_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_id, (Bild,Label) in enumerate(train_loader):\n",
    "        Bild = Bild.to(device)\n",
    "        Label = Label.to(device)\n",
    "\n",
    "        OutputEncoder = trained_encoder(Bild)\n",
    "        OutputEncoder = OutputEncoder.to(device)\n",
    "        OutputEncoder = OutputEncoder.view(OutputEncoder.size(0),-1)\n",
    "\n",
    "        output = Model_Classifier(OutputEncoder)\n",
    "\n",
    "        loss = criterion(output,Label)\n",
    "\n",
    "        if batch_id %512 == 0:\n",
    "            print('Image:',batch_id+1,'of',60000/batch_size,'in Epoche:',epoch+1,'// Loss: %.5f ' % loss)\n",
    "            # view_classifications(Bild[0].cpu().view(1,28,28),output[0])\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Epoche:',epoch+1, 'Loss: %.5f ' % loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0 Save Model Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Model_Classifier,PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusuion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_list = torch.zeros(batch_size)\n",
    "# pred_list = torch.zeros(batch_size)\n",
    "\n",
    "# print(label_list, pred_list)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for data,target in test_dataset:\n",
    "#         data = data.reshape(-1,28,28).to(device)\n",
    "#         target = target.to(device)\n",
    "\n",
    "#         outconf= model1(data)\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = torch.load(FILE)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch_id, (data,target) in enumerate(test_loader):\n",
    "#         data = data.reshape(-1,28*28).to(device)  #[100, 1,28,28] > [100,734] ?\n",
    "#         target = target.to(device)\n",
    "        \n",
    "#         output = model1(data) #was macht out?\n",
    "#         loss = criterion(output,target) #hier out?!\n",
    "\n",
    "#         view_classifications(data[0].view(1,28,28),output[0])\n",
    "#         print('Batch in Epoche ',batch_id+1, 'Loss: %.5f ' % loss)\n",
    "\n",
    "#     print('Epoche:',epoch+1, 'Loss: %.5f ' % loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

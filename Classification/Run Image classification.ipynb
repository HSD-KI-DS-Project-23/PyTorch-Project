{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Von Maximilian Horster und Yanik Plutte"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Generall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is available: False \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Cuda is available:',torch.cuda.is_available(),'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size = (8*4*4)\n",
    "hidden_dim = 128\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "num_epochs = 3\n",
    "learn_rate = 0.0002\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder structure if it does not exist\n",
    "folder_path = \"../../data/\"\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "\n",
    "#MNIST Dataset\n",
    "    #Create Folder, transorm to Tensor\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=folder_path, train=True, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.01307),(0.3081))]), download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=folder_path, train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.01307),(0.3081))])\n",
    ")\n",
    "\n",
    "#Data Loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True #use shuffle to not let the network train on the order of data\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "#show MNIST Dataset\n",
    "\n",
    "# figure= plt.figure(figsize=(10,8))\n",
    "# spalten, zeilen = 5,5\n",
    "# for i in range(1,spalten*zeilen+1):\n",
    "#     sample_idx = torch.randint(len(train_dataset),size=(1,)).item()\n",
    "#     image , label = train_dataset[sample_idx]\n",
    "#     figure.add_subplot(spalten,zeilen,i)\n",
    "#     plt.title(label, weight='bold')\n",
    "#     plt.imshow(image.squeeze(), cmap='gray')\n",
    "# plt.subplots_adjust(wspace=0.3,hspace=0.6)\n",
    "# plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary models for classification\n",
    "    # reduce complexitiy and mass of code\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from classes.autoencoder.autoencoderCNN import AutoEncoder\n",
    "from classes.imageclassification.classification import MNIST_Classification_Class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Load AE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define AutoEncoder Model and use encoder\n",
    "\n",
    "ModelAE = AutoEncoder().to(device)\n",
    "\n",
    "ModelAE.eval()\n",
    "\n",
    "trained_encoder = ModelAE.encode\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Load Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define Image classification model \n",
    "\n",
    "ModelClassification = MNIST_Classification_Class(input_size, hidden_dim).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  6.0 Function: View Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_classifications(img,pred):\n",
    "    pred = pred.cpu().data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(figsize=(6,9), ncols=2) \n",
    "    ax1.imshow(img.resize_(1, 28, 28).cpu().numpy().squeeze())\n",
    "    ax1.set_title('Original MNIST Image')\n",
    "    \n",
    "    bars = ax2.barh(np.arange(10), pred)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))               #Create bar graph to display prediction probability\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    for bar, val in zip(bars, pred):\n",
    "        if not (val < 0.3):\n",
    "            ax2.text(val, bar.get_y() + bar.get_height() / 2, round(val, 2), va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 1 of 7500.0 in Epoche: 1 // Loss: 2.30010 \n",
      "Image: 129 of 7500.0 in Epoche: 1 // Loss: 2.30082 \n",
      "Image: 257 of 7500.0 in Epoche: 1 // Loss: 2.30199 \n",
      "Image: 385 of 7500.0 in Epoche: 1 // Loss: 2.29741 \n",
      "Image: 513 of 7500.0 in Epoche: 1 // Loss: 2.30656 \n",
      "Image: 641 of 7500.0 in Epoche: 1 // Loss: 2.27182 \n",
      "Image: 769 of 7500.0 in Epoche: 1 // Loss: 2.10292 \n",
      "Image: 897 of 7500.0 in Epoche: 1 // Loss: 2.24222 \n",
      "Image: 1025 of 7500.0 in Epoche: 1 // Loss: 2.00906 \n",
      "Image: 1153 of 7500.0 in Epoche: 1 // Loss: 2.00071 \n",
      "Image: 1281 of 7500.0 in Epoche: 1 // Loss: 2.12245 \n",
      "Image: 1409 of 7500.0 in Epoche: 1 // Loss: 2.04613 \n",
      "Image: 1537 of 7500.0 in Epoche: 1 // Loss: 2.13687 \n",
      "Image: 1665 of 7500.0 in Epoche: 1 // Loss: 1.97319 \n",
      "Image: 1793 of 7500.0 in Epoche: 1 // Loss: 1.98917 \n",
      "Image: 1921 of 7500.0 in Epoche: 1 // Loss: 1.97414 \n",
      "Image: 2049 of 7500.0 in Epoche: 1 // Loss: 2.16079 \n",
      "Image: 2177 of 7500.0 in Epoche: 1 // Loss: 1.92517 \n",
      "Image: 2305 of 7500.0 in Epoche: 1 // Loss: 2.17604 \n",
      "Image: 2433 of 7500.0 in Epoche: 1 // Loss: 2.03943 \n",
      "Image: 2561 of 7500.0 in Epoche: 1 // Loss: 2.18228 \n",
      "Image: 2689 of 7500.0 in Epoche: 1 // Loss: 2.04233 \n",
      "Image: 2817 of 7500.0 in Epoche: 1 // Loss: 1.95816 \n",
      "Image: 2945 of 7500.0 in Epoche: 1 // Loss: 1.98442 \n",
      "Image: 3073 of 7500.0 in Epoche: 1 // Loss: 2.04500 \n",
      "Image: 3201 of 7500.0 in Epoche: 1 // Loss: 2.07915 \n",
      "Image: 3329 of 7500.0 in Epoche: 1 // Loss: 1.98048 \n",
      "Image: 3457 of 7500.0 in Epoche: 1 // Loss: 2.23166 \n",
      "Image: 3585 of 7500.0 in Epoche: 1 // Loss: 2.11508 \n",
      "Image: 3713 of 7500.0 in Epoche: 1 // Loss: 2.02996 \n",
      "Image: 3841 of 7500.0 in Epoche: 1 // Loss: 2.07716 \n",
      "Image: 3969 of 7500.0 in Epoche: 1 // Loss: 1.94216 \n",
      "Image: 4097 of 7500.0 in Epoche: 1 // Loss: 1.88365 \n",
      "Image: 4225 of 7500.0 in Epoche: 1 // Loss: 2.11425 \n",
      "Image: 4353 of 7500.0 in Epoche: 1 // Loss: 2.15664 \n",
      "Image: 4481 of 7500.0 in Epoche: 1 // Loss: 2.06765 \n",
      "Image: 4609 of 7500.0 in Epoche: 1 // Loss: 1.92085 \n",
      "Image: 4737 of 7500.0 in Epoche: 1 // Loss: 1.96436 \n",
      "Image: 4865 of 7500.0 in Epoche: 1 // Loss: 2.13441 \n",
      "Image: 4993 of 7500.0 in Epoche: 1 // Loss: 2.08378 \n",
      "Image: 5121 of 7500.0 in Epoche: 1 // Loss: 2.14613 \n",
      "Image: 5249 of 7500.0 in Epoche: 1 // Loss: 2.11766 \n",
      "Image: 5377 of 7500.0 in Epoche: 1 // Loss: 2.08206 \n",
      "Image: 5505 of 7500.0 in Epoche: 1 // Loss: 2.00970 \n",
      "Image: 5633 of 7500.0 in Epoche: 1 // Loss: 1.96208 \n",
      "Image: 5761 of 7500.0 in Epoche: 1 // Loss: 1.95925 \n",
      "Image: 5889 of 7500.0 in Epoche: 1 // Loss: 2.03291 \n",
      "Image: 6017 of 7500.0 in Epoche: 1 // Loss: 2.02685 \n",
      "Image: 6145 of 7500.0 in Epoche: 1 // Loss: 2.07170 \n",
      "Image: 6273 of 7500.0 in Epoche: 1 // Loss: 2.13158 \n",
      "Image: 6401 of 7500.0 in Epoche: 1 // Loss: 1.91269 \n",
      "Image: 6529 of 7500.0 in Epoche: 1 // Loss: 1.95586 \n",
      "Image: 6657 of 7500.0 in Epoche: 1 // Loss: 2.14217 \n",
      "Image: 6785 of 7500.0 in Epoche: 1 // Loss: 2.00740 \n",
      "Image: 6913 of 7500.0 in Epoche: 1 // Loss: 1.96345 \n",
      "Image: 7041 of 7500.0 in Epoche: 1 // Loss: 2.02441 \n",
      "Image: 7169 of 7500.0 in Epoche: 1 // Loss: 1.89521 \n",
      "Image: 7297 of 7500.0 in Epoche: 1 // Loss: 1.97662 \n",
      "Image: 7425 of 7500.0 in Epoche: 1 // Loss: 2.08148 \n",
      "Epoche: 1 Loss: 1.91036 \n",
      "Image: 1 of 7500.0 in Epoche: 2 // Loss: 2.07249 \n",
      "Image: 129 of 7500.0 in Epoche: 2 // Loss: 2.40739 \n",
      "Image: 257 of 7500.0 in Epoche: 2 // Loss: 2.01954 \n",
      "Image: 385 of 7500.0 in Epoche: 2 // Loss: 1.87249 \n",
      "Image: 513 of 7500.0 in Epoche: 2 // Loss: 2.03533 \n",
      "Image: 641 of 7500.0 in Epoche: 2 // Loss: 1.89788 \n",
      "Image: 769 of 7500.0 in Epoche: 2 // Loss: 2.17518 \n",
      "Image: 897 of 7500.0 in Epoche: 2 // Loss: 2.05887 \n",
      "Image: 1025 of 7500.0 in Epoche: 2 // Loss: 1.94247 \n",
      "Image: 1153 of 7500.0 in Epoche: 2 // Loss: 1.97389 \n",
      "Image: 1281 of 7500.0 in Epoche: 2 // Loss: 1.87469 \n",
      "Image: 1409 of 7500.0 in Epoche: 2 // Loss: 1.99982 \n",
      "Image: 1537 of 7500.0 in Epoche: 2 // Loss: 2.11223 \n",
      "Image: 1665 of 7500.0 in Epoche: 2 // Loss: 2.00198 \n",
      "Image: 1793 of 7500.0 in Epoche: 2 // Loss: 1.88136 \n",
      "Image: 1921 of 7500.0 in Epoche: 2 // Loss: 2.08191 \n",
      "Image: 2049 of 7500.0 in Epoche: 2 // Loss: 2.01223 \n",
      "Image: 2177 of 7500.0 in Epoche: 2 // Loss: 1.99948 \n",
      "Image: 2305 of 7500.0 in Epoche: 2 // Loss: 2.07873 \n",
      "Image: 2433 of 7500.0 in Epoche: 2 // Loss: 2.03284 \n",
      "Image: 2561 of 7500.0 in Epoche: 2 // Loss: 1.85792 \n",
      "Image: 2689 of 7500.0 in Epoche: 2 // Loss: 1.87820 \n",
      "Image: 2817 of 7500.0 in Epoche: 2 // Loss: 2.02618 \n",
      "Image: 2945 of 7500.0 in Epoche: 2 // Loss: 2.06629 \n",
      "Image: 3073 of 7500.0 in Epoche: 2 // Loss: 2.13284 \n",
      "Image: 3201 of 7500.0 in Epoche: 2 // Loss: 2.03962 \n",
      "Image: 3329 of 7500.0 in Epoche: 2 // Loss: 1.98076 \n",
      "Image: 3457 of 7500.0 in Epoche: 2 // Loss: 2.17347 \n",
      "Image: 3585 of 7500.0 in Epoche: 2 // Loss: 1.92055 \n",
      "Image: 3713 of 7500.0 in Epoche: 2 // Loss: 1.88166 \n",
      "Image: 3841 of 7500.0 in Epoche: 2 // Loss: 1.93305 \n",
      "Image: 3969 of 7500.0 in Epoche: 2 // Loss: 1.90963 \n",
      "Image: 4097 of 7500.0 in Epoche: 2 // Loss: 1.94770 \n",
      "Image: 4225 of 7500.0 in Epoche: 2 // Loss: 1.83737 \n",
      "Image: 4353 of 7500.0 in Epoche: 2 // Loss: 2.00754 \n",
      "Image: 4481 of 7500.0 in Epoche: 2 // Loss: 1.81379 \n",
      "Image: 4609 of 7500.0 in Epoche: 2 // Loss: 1.91523 \n",
      "Image: 4737 of 7500.0 in Epoche: 2 // Loss: 1.94674 \n",
      "Image: 4865 of 7500.0 in Epoche: 2 // Loss: 1.88158 \n",
      "Image: 4993 of 7500.0 in Epoche: 2 // Loss: 1.95328 \n",
      "Image: 5121 of 7500.0 in Epoche: 2 // Loss: 1.92004 \n",
      "Image: 5249 of 7500.0 in Epoche: 2 // Loss: 2.01344 \n",
      "Image: 5377 of 7500.0 in Epoche: 2 // Loss: 2.01527 \n",
      "Image: 5505 of 7500.0 in Epoche: 2 // Loss: 1.80585 \n",
      "Image: 5633 of 7500.0 in Epoche: 2 // Loss: 1.82081 \n",
      "Image: 5761 of 7500.0 in Epoche: 2 // Loss: 2.03917 \n",
      "Image: 5889 of 7500.0 in Epoche: 2 // Loss: 1.97511 \n",
      "Image: 6017 of 7500.0 in Epoche: 2 // Loss: 2.03876 \n",
      "Image: 6145 of 7500.0 in Epoche: 2 // Loss: 1.87834 \n",
      "Image: 6273 of 7500.0 in Epoche: 2 // Loss: 2.10202 \n",
      "Image: 6401 of 7500.0 in Epoche: 2 // Loss: 1.95146 \n",
      "Image: 6529 of 7500.0 in Epoche: 2 // Loss: 1.90404 \n",
      "Image: 6657 of 7500.0 in Epoche: 2 // Loss: 2.08937 \n",
      "Image: 6785 of 7500.0 in Epoche: 2 // Loss: 1.85691 \n",
      "Image: 6913 of 7500.0 in Epoche: 2 // Loss: 1.91851 \n",
      "Image: 7041 of 7500.0 in Epoche: 2 // Loss: 1.86811 \n",
      "Image: 7169 of 7500.0 in Epoche: 2 // Loss: 1.95351 \n",
      "Image: 7297 of 7500.0 in Epoche: 2 // Loss: 1.92174 \n",
      "Image: 7425 of 7500.0 in Epoche: 2 // Loss: 2.08870 \n",
      "Epoche: 2 Loss: 2.03726 \n",
      "Image: 1 of 7500.0 in Epoche: 3 // Loss: 1.87463 \n",
      "Image: 129 of 7500.0 in Epoche: 3 // Loss: 1.82580 \n",
      "Image: 257 of 7500.0 in Epoche: 3 // Loss: 1.78732 \n",
      "Image: 385 of 7500.0 in Epoche: 3 // Loss: 1.78190 \n",
      "Image: 513 of 7500.0 in Epoche: 3 // Loss: 2.08924 \n",
      "Image: 641 of 7500.0 in Epoche: 3 // Loss: 1.90557 \n",
      "Image: 769 of 7500.0 in Epoche: 3 // Loss: 1.92266 \n",
      "Image: 897 of 7500.0 in Epoche: 3 // Loss: 1.81948 \n",
      "Image: 1025 of 7500.0 in Epoche: 3 // Loss: 1.82080 \n",
      "Image: 1153 of 7500.0 in Epoche: 3 // Loss: 1.95226 \n",
      "Image: 1281 of 7500.0 in Epoche: 3 // Loss: 1.76101 \n",
      "Image: 1409 of 7500.0 in Epoche: 3 // Loss: 1.83754 \n",
      "Image: 1537 of 7500.0 in Epoche: 3 // Loss: 1.86281 \n",
      "Image: 1665 of 7500.0 in Epoche: 3 // Loss: 1.83630 \n",
      "Image: 1793 of 7500.0 in Epoche: 3 // Loss: 1.72352 \n",
      "Image: 1921 of 7500.0 in Epoche: 3 // Loss: 1.89917 \n",
      "Image: 2049 of 7500.0 in Epoche: 3 // Loss: 1.81000 \n",
      "Image: 2177 of 7500.0 in Epoche: 3 // Loss: 1.89871 \n",
      "Image: 2305 of 7500.0 in Epoche: 3 // Loss: 2.24863 \n",
      "Image: 2433 of 7500.0 in Epoche: 3 // Loss: 1.75247 \n",
      "Image: 2561 of 7500.0 in Epoche: 3 // Loss: 1.93038 \n",
      "Image: 2689 of 7500.0 in Epoche: 3 // Loss: 1.84808 \n",
      "Image: 2817 of 7500.0 in Epoche: 3 // Loss: 1.78672 \n",
      "Image: 2945 of 7500.0 in Epoche: 3 // Loss: 2.00543 \n",
      "Image: 3073 of 7500.0 in Epoche: 3 // Loss: 1.83677 \n",
      "Image: 3201 of 7500.0 in Epoche: 3 // Loss: 1.99626 \n",
      "Image: 3329 of 7500.0 in Epoche: 3 // Loss: 1.86035 \n",
      "Image: 3457 of 7500.0 in Epoche: 3 // Loss: 1.73145 \n",
      "Image: 3585 of 7500.0 in Epoche: 3 // Loss: 1.79967 \n",
      "Image: 3713 of 7500.0 in Epoche: 3 // Loss: 1.76589 \n",
      "Image: 3841 of 7500.0 in Epoche: 3 // Loss: 1.85320 \n",
      "Image: 3969 of 7500.0 in Epoche: 3 // Loss: 1.72675 \n",
      "Image: 4097 of 7500.0 in Epoche: 3 // Loss: 1.84071 \n",
      "Image: 4225 of 7500.0 in Epoche: 3 // Loss: 1.76025 \n",
      "Image: 4353 of 7500.0 in Epoche: 3 // Loss: 1.85049 \n",
      "Image: 4481 of 7500.0 in Epoche: 3 // Loss: 1.90023 \n",
      "Image: 4609 of 7500.0 in Epoche: 3 // Loss: 1.81748 \n",
      "Image: 4737 of 7500.0 in Epoche: 3 // Loss: 1.87330 \n",
      "Image: 4865 of 7500.0 in Epoche: 3 // Loss: 1.82244 \n",
      "Image: 4993 of 7500.0 in Epoche: 3 // Loss: 1.68226 \n",
      "Image: 5121 of 7500.0 in Epoche: 3 // Loss: 1.80399 \n",
      "Image: 5249 of 7500.0 in Epoche: 3 // Loss: 1.70723 \n",
      "Image: 5377 of 7500.0 in Epoche: 3 // Loss: 1.85510 \n",
      "Image: 5505 of 7500.0 in Epoche: 3 // Loss: 1.79303 \n",
      "Image: 5633 of 7500.0 in Epoche: 3 // Loss: 1.76008 \n",
      "Image: 5761 of 7500.0 in Epoche: 3 // Loss: 2.14640 \n",
      "Image: 5889 of 7500.0 in Epoche: 3 // Loss: 1.95318 \n",
      "Image: 6017 of 7500.0 in Epoche: 3 // Loss: 1.92044 \n",
      "Image: 6145 of 7500.0 in Epoche: 3 // Loss: 1.83919 \n",
      "Image: 6273 of 7500.0 in Epoche: 3 // Loss: 1.76858 \n",
      "Image: 6401 of 7500.0 in Epoche: 3 // Loss: 2.00272 \n",
      "Image: 6529 of 7500.0 in Epoche: 3 // Loss: 1.66841 \n",
      "Image: 6657 of 7500.0 in Epoche: 3 // Loss: 1.99800 \n",
      "Image: 6785 of 7500.0 in Epoche: 3 // Loss: 1.96069 \n",
      "Image: 6913 of 7500.0 in Epoche: 3 // Loss: 1.78776 \n",
      "Image: 7041 of 7500.0 in Epoche: 3 // Loss: 1.80447 \n",
      "Image: 7169 of 7500.0 in Epoche: 3 // Loss: 1.87474 \n",
      "Image: 7297 of 7500.0 in Epoche: 3 // Loss: 1.90320 \n",
      "Image: 7425 of 7500.0 in Epoche: 3 // Loss: 1.79989 \n",
      "Epoche: 3 Loss: 1.90033 \n"
     ]
    }
   ],
   "source": [
    "#Load Model / Continue Training\n",
    "continue_training = False\n",
    "\n",
    "FILE_model = \"Classification Model.pth\"\n",
    "\n",
    "if continue_training:\n",
    "    ModelClassification = torch.load(FILE_Model1).to(device)\n",
    "\n",
    "#criterion / lf\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer = optim.Adam(params=ModelClassification.parameters(),lr=learn_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_id, (Bild,Label) in enumerate(train_loader):\n",
    "        Bild = Bild.to(device)\n",
    "        Label = Label.to(device)\n",
    "\n",
    "        outputAE = trained_encoder(Bild)\n",
    "        outputAE = outputAE.to(device)\n",
    "        outputAE = outputAE.view(outputAE.size(0),-1)\n",
    "\n",
    "        output = ModelClassification(outputAE)\n",
    "\n",
    "        loss = criterion(output,Label)\n",
    "\n",
    "        if batch_id %128 == 0:\n",
    "            print('Image:',batch_id+1,'of',60000/batch_size,'in Epoche:',epoch+1,'// Loss: %.5f ' % loss)\n",
    "            # view_classifications(Bild[0].cpu().view(1,28,28),output[0])\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Epoche:',epoch+1, 'Loss: %.5f ' % loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0 Save Model Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(MNIST_Classification_Class,FILE_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusuion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_list = torch.zeros(batch_size)\n",
    "# pred_list = torch.zeros(batch_size)\n",
    "\n",
    "# print(label_list, pred_list)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for data,target in test_dataset:\n",
    "#         data = data.reshape(-1,28,28).to(device)\n",
    "#         target = target.to(device)\n",
    "\n",
    "#         outconf= model1(data)\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = torch.load(FILE)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch_id, (data,target) in enumerate(test_loader):\n",
    "#         data = data.reshape(-1,28*28).to(device)  #[100, 1,28,28] > [100,734] ?\n",
    "#         target = target.to(device)\n",
    "        \n",
    "#         output = model1(data) #was macht out?\n",
    "#         loss = criterion(output,target) #hier out?!\n",
    "\n",
    "#         view_classifications(data[0].view(1,28,28),output[0])\n",
    "#         print('Batch in Epoche ',batch_id+1, 'Loss: %.5f ' % loss)\n",
    "\n",
    "#     print('Epoche:',epoch+1, 'Loss: %.5f ' % loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
